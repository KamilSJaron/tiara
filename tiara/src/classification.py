import gzip
from typing import Dict, Union, List, Generator
import warnings
from contextlib import suppress

from Bio.SeqIO.FastaIO import SimpleFastaParser
from tqdm import tqdm
from joblib import Parallel, delayed

from skorch import NeuralNetClassifier
import torch

from tiara.src.prediction import Prediction, SingleResult
from tiara.src.models import NNet1, NNet2
from tiara.src.transformations import Transformer, TfidfWeighter
from tiara.src.utilities import parse_params, chop, time_context_manager


def fun(seq, layer, transformer, fragment_len):
    chopped = chop(seq, fragment_len)
    return transformer.transform(chopped)


allowed_letters = set("ACGT")


class Classification:
    """Class that performs classification given neural net and tf-idf models provided.

    Methods
    -------
        classify: classifies an entire fasta file
    """

    def __init__(
        self,
        min_len: int,
        nnet_weights: List[str],
        params: List[Union[Dict[str, int], str]],
        tfidf: List[str],
        threads: int = 1,
        models=(NNet1, NNet2),
    ):
        """Init method.

        Parameters
        ----------
            min_len: minimal length of the sequence to classify
            nnet_weights: a list of paths to nnet weights (generated by skorch for the nnet provided)
            params: either a list of dicts containing neural net params such as number of nodes in hidden layers
                and other parameters such as fragment lenth, kmer length etc,
                or a list of filepaths to .csv files containing parameters
                refer to utilities.parse_params docstring for a specific example
            tfidf: a list of filepaths to tf-idf model
            models: an iterable of torch model classes describing model used
        """
        self.threads = threads
        self.params = [
            parse_params(param) if isinstance(param, str) else param for param in params
        ]
        self.nnets = []
        for model, nnet_weight, params_dict in zip(models, nnet_weights, self.params):
            params_filtered = {
                key: value
                for key, value in params_dict.items()
                if key not in ["k", "fragment_len", "prob_cutoff", "fname"]
            }
            params_filtered.update({"dim_in": 4 ** params_dict["k"]})
            module = model(**params_filtered)
            net = NeuralNetClassifier(module, lr=0.0001, criterion=torch.nn.NLLLoss)
            net.initialize()
            net.load_params(f_params=nnet_weight)
            self.nnets.append(net)
        self.transformers = [
            TfidfWeighter.load_params(tfidf_param_folder)
            for tfidf_param_folder in tfidf
        ]
        self.min_len = min_len
        self.layers = len(nnet_weights)
        self.predictors = [
            Prediction(
                prob_cutoff=self.params[layer]["prob_cutoff"],
                layer=layer,
                nnet=self.nnets[layer],
                fragment_len=self.params[layer]["fragment_len"],
                k=self.params[layer]["k"],
                tnf=self.transformers[layer],
                transformer=Transformer(
                    fragment_len=self.params[layer]["fragment_len"],
                    k=self.params[layer]["k"],
                    model=self.transformers[layer],
                ),
            )
            for layer in range(self.layers)
        ]

    def _sequence_stream(self, sequences_fname: str):
        """Yield (header, sequence) tuples for each sequence in file above min_len."""
        ParseFunc = SimpleFastaParser
        opener = gzip.open if sequences_fname.endswith(".gz") else open
        mode = "rt" if sequences_fname.endswith(".gz") else "r"
        with opener(sequences_fname, mode) as sequences_handle:
            for header, seq in ParseFunc(sequences_handle):
                if len(seq) >= self.min_len:
                    yield (header, seq)

    def classify(self, sequences_fname: str, verbose=False) -> List[SingleResult]:
        """Perform a two-step classification (streamed, low memory).

        Parameters
        ----------
            sequences_fname : a path to fasta file to classify

        Returns
        -------
            predictions: a list of lists containing SingleResult objects.
        """
        # For progress bar, count sequences if verbose
        if verbose:
            with (gzip.open(sequences_fname, "rt") if sequences_fname.endswith(".gz") else open(sequences_fname, "r")) as handle:
                n_seqs = sum(1 for line in handle if line.startswith(">"))
            seq_iter = self._sequence_stream(sequences_fname)
            seq_iter = tqdm(seq_iter, total=n_seqs, desc="Classifying")
        else:
            seq_iter = self._sequence_stream(sequences_fname)

        predictions = []
        to_second_stage = []
        for header, seq in seq_iter:
            # First stage: feature extraction and classification
            features = fun(seq, 0, self.predictors[0].transformer, self.params[0]["fragment_len"])
            seq_item = (header, seq, features)
            pred1 = self.predictors[0].make_prediction(seq_item)
            if pred1.cls[0] == "organelle":
                to_second_stage.append(pred1)
            else:
                predictions.append(pred1)

        # Stream second stage for organellar sequences
        if to_second_stage:
            if verbose:
                print("Performing second stage of classification.")
                seq_iter2 = tqdm(to_second_stage, desc="Second stage")
            else:
                seq_iter2 = to_second_stage
            for record in seq_iter2:
                features2 = fun(
                    record.seq, 1, self.predictors[1].transformer, self.params[1]["fragment_len"]
                )
                seq_item2 = (record.desc, record.seq, features2)
                pred2 = self.predictors[1].make_prediction(seq_item2)
                assert record.desc == pred2.desc, "Descriptions not the same"
                assert record.seq == pred2.seq, "Sequences not the same"
                predictions.append(
                    SingleResult(
                        desc=record.desc,
                        seq=record.seq,
                        cls=[record.cls[0], pred2.cls[1]],
                        probs=[record.probs[0], pred2.probs[1]],
                    )
                )
        return predictions
